"""
Realtime Announcement Session - Conversa OpenAI Realtime com humano durante transfer√™ncia.

Permite que o agente IA converse por voz com o atendente humano,
oferecendo uma experi√™ncia mais natural que TTS + DTMF.

Ref: voice-ai-ivr/openspec/changes/announced-transfer/
"""

import asyncio
import base64
import json
import logging
import os
from dataclasses import dataclass
from typing import Optional

import websockets
from websockets.asyncio.client import ClientConnection
from websockets.asyncio.server import ServerConnection

from .esl_client import AsyncESLClient
from ..utils.resampler import Resampler, AudioBuffer

logger = logging.getLogger(__name__)

# Configura√ß√µes OpenAI Realtime (GA)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
# Modelo GA (General Availability) - Recomendado
OPENAI_REALTIME_MODEL = os.getenv("OPENAI_REALTIME_MODEL", "gpt-realtime")
# Vozes v√°lidas GA: alloy, ash, ballad, coral, echo, sage, shimmer, verse, marin, cedar
OPENAI_REALTIME_VOICE = os.getenv("OPENAI_REALTIME_VOICE", "marin")


@dataclass
class AnnouncementResult:
    """Resultado da conversa de an√∫ncio com o humano."""
    accepted: bool = False
    rejected: bool = False
    message: Optional[str] = None
    transcript: str = ""
    duration_seconds: float = 0.0


class RealtimeAnnouncementSession:
    """
    Sess√£o OpenAI Realtime para conversar com humano durante transfer√™ncia.
    
    Fluxo:
    1. Conectar ao OpenAI Realtime
    2. Configurar sess√£o com prompt de an√∫ncio
    3. Iniciar stream de √°udio do B-leg (via mod_audio_stream)
    4. Enviar mensagem inicial de an√∫ncio
    5. Processar respostas do humano
    6. Detectar aceita√ß√£o/recusa
    7. Retornar resultado
    
    Uso:
        session = RealtimeAnnouncementSession(
            b_leg_uuid="xxx",
            esl_client=esl,
            system_prompt="Voc√™ est√° anunciando...",
            initial_message="Tenho o Jo√£o na linha sobre planos..."
        )
        result = await session.run(timeout=15.0)
        
        if result.accepted:
            # Fazer bridge
        elif result.rejected:
            # Voltar para cliente com mensagem
    """
    
    def __init__(
        self,
        b_leg_uuid: str,
        esl_client: AsyncESLClient,
        system_prompt: str,
        initial_message: str,
        voice: str = OPENAI_REALTIME_VOICE,
        model: str = OPENAI_REALTIME_MODEL,
    ):
        """
        Args:
            b_leg_uuid: UUID do B-leg (humano)
            esl_client: Cliente ESL para controle de √°udio
            system_prompt: Prompt de sistema para o agente
            initial_message: Mensagem inicial de an√∫ncio
            voice: Voz do OpenAI GA (alloy, ash, ballad, coral, echo, sage, shimmer, verse, marin, cedar)
            model: Modelo Realtime
        """
        self.b_leg_uuid = b_leg_uuid
        self.esl = esl_client
        self.system_prompt = system_prompt
        self.initial_message = initial_message
        self.voice = voice
        self.model = model
        
        self._ws: Optional[ClientConnection] = None
        self._running = False
        self._transcript = ""
        self._accepted = False
        self._rejected = False
        self._rejection_message: Optional[str] = None
        
        # WebSocket URL para receber √°udio do FreeSWITCH
        self._audio_ws_server: Optional[asyncio.Server] = None
        self._audio_ws_port: int = 0
        self._fs_ws: Optional[ServerConnection] = None
        self._fs_connected = asyncio.Event()
        self._fs_sender_task: Optional[asyncio.Task] = None
        self._fs_audio_queue: asyncio.Queue[bytes] = asyncio.Queue(maxsize=200)
        self._fs_rawaudio_sent = False
        
        # Resamplers: FS 16kHz <-> OpenAI 24kHz
        self._resampler_in = Resampler(16000, 24000)
        self._resampler_out = Resampler(24000, 16000)
        self._fs_audio_buffer = AudioBuffer(warmup_ms=300, sample_rate=16000)
    
    async def run(self, timeout: float = 15.0) -> AnnouncementResult:
        """
        Executa a conversa de an√∫ncio.
        
        Args:
            timeout: Tempo m√°ximo de conversa em segundos
        
        Returns:
            AnnouncementResult com decis√£o do humano
        """
        import time
        start_time = time.time()
        
        # Debug: Log in√≠cio da sess√£o
        logger.info("=" * 60)
        logger.info("üé§ REALTIME ANNOUNCEMENT STARTING")
        logger.info(f"B-leg UUID: {self.b_leg_uuid}")
        logger.info(f"Model: {self.model}")
        logger.info(f"Voice: {self.voice}")
        logger.info(f"Timeout: {timeout}s")
        logger.info(f"Initial message: {self.initial_message[:100] if self.initial_message else 'None'}...")
        logger.info("=" * 60)
        
        try:
            self._running = True
            
            # 1. Conectar ao OpenAI Realtime
            logger.info("üîå Step 1: Connecting to OpenAI Realtime...")
            await self._connect_openai()
            logger.info("‚úÖ Step 1: Connected to OpenAI Realtime")
            
            # 2. Configurar sess√£o
            logger.info("‚öôÔ∏è Step 2: Configuring session...")
            await self._configure_session()
            logger.info("‚úÖ Step 2: Session configured")
            
            # 3. Iniciar stream de √°udio do FreeSWITCH
            logger.info("üé§ Step 3: Starting audio stream...")
            await self._start_audio_stream()
            logger.info("‚úÖ Step 3: Audio stream started")
            
            # 4. Enviar mensagem inicial
            logger.info("üí¨ Step 4: Sending initial message...")
            await self._send_initial_message()
            logger.info("‚úÖ Step 4: Initial message sent")
            
            # 5. Loop principal - processar eventos at√© decis√£o ou timeout
            logger.info("‚ñ∂Ô∏è Step 5: Starting event loop...")
            await asyncio.wait_for(
                self._event_loop(),
                timeout=timeout
            )
            
        except asyncio.TimeoutError:
            logger.info("Realtime announcement timeout")
        
        except asyncio.CancelledError:
            logger.info("Realtime announcement cancelled")
            raise
        
        except Exception as e:
            logger.exception(f"Realtime announcement error: {e}")
        
        finally:
            self._running = False
            await self._cleanup()
        
        duration = time.time() - start_time
        
        return AnnouncementResult(
            accepted=self._accepted,
            rejected=self._rejected,
            message=self._rejection_message,
            transcript=self._transcript,
            duration_seconds=duration,
        )
    
    async def _connect_openai(self) -> None:
        """Conecta ao WebSocket do OpenAI Realtime (GA)."""
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY not configured")
        
        url = f"wss://api.openai.com/v1/realtime?model={self.model}"
        
        # Headers para API GA - OpenAI-Beta n√£o √© mais necess√°rio
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
        }
        
        # Fallback para modelos preview (deprecated)
        if "preview" in self.model.lower():
            headers["OpenAI-Beta"] = "realtime=v1"
            logger.warning(f"Using preview model - consider migrating to gpt-realtime")
        
        self._ws = await websockets.connect(
            url,
            additional_headers=headers,
            ping_interval=20,
            ping_timeout=10,
        )
        
        # Aguardar session.created
        msg = await asyncio.wait_for(self._ws.recv(), timeout=5.0)
        event = json.loads(msg)
        
        if event.get("type") != "session.created":
            raise RuntimeError(f"Expected session.created, got {event.get('type')}")
        
        logger.info(f"Connected to OpenAI Realtime (GA) for announcement - model={self.model}")
    
    async def _configure_session(self) -> None:
        """
        Configura a sess√£o OpenAI Realtime.
        
        FORMATO GA (gpt-realtime):
        - Campos DENTRO de "session" wrapper
        - Ref: https://platform.openai.com/docs/api-reference/realtime
        
        NOTA: Para an√∫ncios curtos, usamos semantic_vad com eagerness=high
        para responder rapidamente quando o humano aceitar/recusar.
        """
        # FORMATO GA - Usar mesma estrutura do provider principal que funciona
        # Estrutura aninhada com audio.input e audio.output
        config = {
            "type": "session.update",
            "session": {
                # Tipo de sess√£o (OBRIGAT√ìRIO na API GA)
                "type": "realtime",
                
                # Modalidades de sa√≠da
                "output_modalities": ["audio"],
                
                # Instru√ß√µes do sistema
                "instructions": self.system_prompt,
                
                # Configura√ß√£o de √°udio (estrutura aninhada)
                "audio": {
                    "input": {
                        "format": {
                            "type": "audio/pcm",
                            "rate": 24000
                        },
                        "noise_reduction": {"type": "far_field"},
                        # VAD para detectar quando humano fala
                        "turn_detection": {
                            "type": "server_vad",
                            "threshold": 0.5,
                            "prefix_padding_ms": 300,
                            "silence_duration_ms": 500
                        },
                        # Transcri√ß√£o do que o humano fala
                        "transcription": {
                            "model": "gpt-4o-transcribe"
                        },
                    },
                    "output": {
                        "format": {
                            "type": "audio/pcm",
                            "rate": 24000
                        },
                        "voice": self.voice,
                    },
                },
            }
        }
        
        logger.debug(f"Sending session config: {json.dumps(config)[:500]}")
        
        await self._ws.send(json.dumps(config))
        
        # Aguardar confirma√ß√£o session.updated
        try:
            msg = await asyncio.wait_for(self._ws.recv(), timeout=5.0)
            event = json.loads(msg)
            if event.get("type") == "session.updated":
                logger.info("‚úÖ Session configured successfully (server_vad)")
            elif event.get("type") == "error":
                error = event.get("error", {})
                logger.error(f"‚ùå Session config error: {error}")
                # N√£o levantar exce√ß√£o - tentar continuar mesmo assim
                # raise RuntimeError(f"Session config failed: {error.get('message', 'unknown')}")
            else:
                logger.warning(f"‚ö†Ô∏è Unexpected event after session.update: {event.get('type')}")
        except asyncio.TimeoutError:
            logger.warning("‚ö†Ô∏è No session.updated confirmation received (timeout 5s), continuing...")
    
    async def _start_audio_stream(self) -> None:
        """
        Inicia stream de √°udio bidirecional entre FreeSWITCH e OpenAI.
        
        ‚ö†Ô∏è LIMITA√á√ÉO ATUAL:
        A implementa√ß√£o completa de stream bidirecional requer:
        1. WebSocket server intermedi√°rio
        2. mod_audio_stream conectado a esse server
        3. Bridge WS <-> OpenAI Realtime
        
        IMPLEMENTA√á√ÉO ATUAL (Simplificada):
        - OpenAI ‚Üí FreeSWITCH: via _play_audio (arquivo tempor√°rio + uuid_broadcast)
        - FreeSWITCH ‚Üí OpenAI: ‚ö†Ô∏è N√ÉO IMPLEMENTADO
        
        Por isso, o Realtime Announcement funciona de forma SEMI-DUPLEX:
        1. Assistente fala o an√∫ncio (gerado via text ‚Üí OpenAI ‚Üí TTS)
        2. Sistema detecta resposta do humano via:
           - Patterns de texto simples se o humano falar
           - Timeout para aceitar automaticamente
        
        Para implementa√ß√£o FULL-DUPLEX futura:
        - Usar mod_audio_stream com WebSocket intermedi√°rio
        - Ou usar FreeSWITCH com WebRTC + SRTP
        """
        try:
            # Inicializar buffer de √°udio para fallback TTS
            self._audio_buffer = bytearray()
            self._human_transcript = ""
            
            # 1) Subir WS server para receber √°udio do B-leg
            # bind_host: onde o server escuta (0.0.0.0 para todas interfaces)
            # connect_host: como o FreeSWITCH vai conectar (IP externo ou hostname)
            bind_host = os.getenv("REALTIME_BLEG_STREAM_BIND", "0.0.0.0")
            connect_host = os.getenv("REALTIME_BLEG_STREAM_HOST", "host.docker.internal")
            
            logger.info(f"üîä Starting B-leg audio WS server on {bind_host}...")
            
            self._audio_ws_server = await websockets.serve(
                self._handle_fs_ws,
                bind_host,
                0,  # Porta aleat√≥ria
                max_size=None,
            )
            if not self._audio_ws_server.sockets:
                raise RuntimeError("Failed to allocate port for B-leg audio WS")
            
            self._audio_ws_port = self._audio_ws_server.sockets[0].getsockname()[1]
            ws_url = f"ws://{connect_host}:{self._audio_ws_port}/bleg/{self.b_leg_uuid}"
            
            logger.info(f"üîä B-leg audio WS server ready at {ws_url}")
            
            # 2) Iniciar mod_audio_stream no B-leg
            # Usa conex√£o ESL dedicada para evitar conflitos com o singleton
            cmd = f"uuid_audio_stream {self.b_leg_uuid} start {ws_url} mono 16k"
            logger.info(f"üîä Executing ESL command: {cmd}")
            
            try:
                # Criar conex√£o ESL dedicada para este comando
                dedicated_esl = AsyncESLClient()
                try:
                    connected = await asyncio.wait_for(
                        dedicated_esl.connect(),
                        timeout=2.0
                    )
                    if not connected:
                        raise RuntimeError("Failed to connect dedicated ESL")
                    
                    logger.debug("üîä Dedicated ESL connected for uuid_audio_stream")
                    
                    response = await asyncio.wait_for(
                        dedicated_esl.execute_api(cmd),
                        timeout=3.0
                    )
                    logger.info(
                        f"üîä B-leg audio stream command result: {response[:200] if response else 'None'}",
                        extra={
                            "b_leg_uuid": self.b_leg_uuid,
                            "ws_url": ws_url,
                            "esl_response": response,
                        },
                    )
                finally:
                    await dedicated_esl.disconnect()
                    
            except asyncio.TimeoutError:
                logger.error(f"‚ùå ESL command timeout: {cmd}")
            except Exception as e:
                logger.error(f"‚ùå ESL command failed: {e}")
            
            # 3) Aguardar conex√£o do FreeSWITCH
            try:
                await asyncio.wait_for(self._fs_connected.wait(), timeout=5.0)
                logger.info(
                    "‚úÖ B-leg audio stream connected (FULL-DUPLEX)",
                    extra={"b_leg_uuid": self.b_leg_uuid},
                )
            except asyncio.TimeoutError:
                logger.warning(
                    "‚ö†Ô∏è B-leg audio stream did not connect - SEMI-DUPLEX mode (TTS fallback)",
                    extra={"b_leg_uuid": self.b_leg_uuid},
                )
                # N√ÉO retorna - continua em modo semi-duplex (TTS fallback)
            
        except Exception as e:
            logger.error(f"Failed to initialize audio stream: {e}")
    
    async def _send_initial_message(self) -> None:
        """Envia mensagem inicial de an√∫ncio."""
        if not self._ws:
            logger.error("Cannot send initial message: WebSocket not connected")
            return
        
        # Criar item de conversa com a mensagem inicial
        await self._ws.send(json.dumps({
            "type": "conversation.item.create",
            "item": {
                "type": "message",
                "role": "user",
                "content": [{"type": "input_text", "text": self.initial_message}]
            }
        }))
        
        # Solicitar resposta
        await self._ws.send(json.dumps({"type": "response.create"}))
        
        logger.info(f"Initial announcement sent: {self.initial_message[:50]}...")
    
    async def _event_loop(self) -> None:
        """Loop principal de processamento de eventos."""
        while self._running and not self._accepted and not self._rejected:
            try:
                msg = await asyncio.wait_for(self._ws.recv(), timeout=1.0)
                event = json.loads(msg)
                await self._handle_event(event)
                
            except asyncio.TimeoutError:
                # Verificar se B-leg ainda existe (com timeout curto para n√£o bloquear)
                try:
                    b_leg_exists = await asyncio.wait_for(
                        self.esl.uuid_exists(self.b_leg_uuid),
                        timeout=1.0
                    )
                    if not b_leg_exists:
                        logger.info("B-leg hangup detected")
                        self._rejected = True
                        self._rejection_message = "Humano desligou"
                        break
                except (asyncio.TimeoutError, Exception) as e:
                    # ESL check falhou - n√£o assumir hangup, continuar
                    logger.debug(f"B-leg check failed (continuing): {e}")
    
    async def _handle_event(self, event: dict) -> None:
        """Processa evento do OpenAI Realtime."""
        etype = event.get("type", "")
        
        # √Åudio de resposta do assistente - enviar para FreeSWITCH (B-leg/humano)
        if etype in ("response.audio.delta", "response.output_audio.delta"):
            audio_b64 = event.get("delta", "")
            if audio_b64:
                audio_bytes = base64.b64decode(audio_b64)
                if self._fs_ws:
                    await self._enqueue_audio_to_freeswitch(audio_bytes)
                else:
                    await self._play_audio(audio_bytes)
        
        # Transcri√ß√£o do HUMANO (input) - IMPORTANTE: √© aqui que detectamos aceite/recusa
        elif etype == "conversation.item.input_audio_transcription.completed":
            human_transcript = event.get("transcript", "")
            logger.info(f"Human said: {human_transcript}")
            
            # Atualizar transcript do humano
            if not hasattr(self, '_human_transcript'):
                self._human_transcript = ""
            self._human_transcript += human_transcript + " "
            
            # Verificar decis√£o baseada no que o HUMANO disse
            self._check_human_decision(human_transcript)
        
        # Transcri√ß√£o do assistente (para log)
        elif etype in ("response.audio_transcript.delta", "response.output_audio_transcript.delta"):
            delta = event.get("delta", "")
            self._transcript += delta
        
        # Resposta completa do assistente
        elif etype == "response.done":
            response = event.get("response", {})
            status = response.get("status", "completed")
            logger.debug(f"Response complete (status={status}), transcript: {self._transcript[-100:]}")
            
            # Verificar se assistente decidiu
            self._check_decision()
        
        # Erro
        elif etype == "error":
            error = event.get("error", {})
            error_code = error.get("code", "unknown")
            # Ignorar erros n√£o-cr√≠ticos
            if error_code not in ("response_cancel_not_active",):
                logger.error(f"OpenAI error: {error}")
    
    def _check_human_decision(self, human_text: str) -> None:
        """
        Verifica decis√£o baseada no que o HUMANO disse.
        
        Detecta frases de:
        - Aceite: "pode passar", "pode transferir", "ok", "sim"
        - Recusa: "n√£o posso", "estou ocupado", "depois", "recuso"
        
        Args:
            human_text: Transcri√ß√£o do que o humano falou
        """
        text_lower = human_text.lower().strip()
        
        # Palavras/frases de ACEITE
        accept_patterns = [
            "pode passar", "pode transferir", "pode conectar",
            "ok", "t√° bom", "t√° bem", "beleza",
            "sim", "claro", "certo", "pode",
            "manda", "passa a√≠", "conecta",
        ]
        
        # Palavras/frases de RECUSA
        reject_patterns = [
            "n√£o posso", "n√£o d√°", "n√£o",
            "estou ocupado", "ocupado", "em reuni√£o",
            "depois", "mais tarde", "agora n√£o",
            "recuso", "n√£o quero", "n√£o tenho tempo",
        ]
        
        # Verificar aceite
        for pattern in accept_patterns:
            if pattern in text_lower:
                self._accepted = True
                logger.info(f"Human ACCEPTED: matched '{pattern}' in '{text_lower[:50]}'")
                return
        
        # Verificar recusa
        for pattern in reject_patterns:
            if pattern in text_lower:
                self._rejected = True
                self._rejection_message = human_text
                logger.info(f"Human REJECTED: matched '{pattern}' in '{text_lower[:50]}'")
                return
    
    def _check_decision(self) -> None:
        """
        Verifica se a transcri√ß√£o do ASSISTENTE cont√©m decis√£o.
        
        O assistente deve responder com "ACEITO" ou "RECUSADO: [motivo]"
        ap√≥s interpretar a resposta do humano.
        """
        text = self._transcript.upper()
        
        if "ACEITO" in text:
            self._accepted = True
            logger.info("Decision from assistant: ACCEPTED")
        
        elif "RECUSADO" in text:
            self._rejected = True
            # Extrair mensagem ap√≥s "RECUSADO:"
            parts = self._transcript.split("RECUSADO:")
            if len(parts) > 1:
                self._rejection_message = parts[1].strip()[:200]
            logger.info(f"Decision detected: REJECTED - {self._rejection_message}")
    
    async def _play_audio(self, audio_bytes: bytes) -> None:
        """
        Envia √°udio para o B-leg via FreeSWITCH.
        
        OpenAI Realtime retorna PCM16 @ 24kHz.
        FreeSWITCH precisa de WAV com header ou convers√£o.
        
        Estrat√©gia: Acumular chunks e tocar via uuid_broadcast.
        """
        # Acumular √°udio no buffer
        if not hasattr(self, '_audio_buffer'):
            self._audio_buffer = bytearray()
        
        self._audio_buffer.extend(audio_bytes)
        
        # Tocar quando tiver √°udio suficiente (~500ms = 24000 samples @ 24kHz)
        MIN_BUFFER_SIZE = 24000  # 0.5s de √°udio @ 24kHz PCM16
        
        if len(self._audio_buffer) >= MIN_BUFFER_SIZE:
            await self._flush_audio_buffer()
    
    async def _flush_audio_buffer(self) -> None:
        """
        Toca √°udio acumulado no buffer.
        
        Usa asyncio subprocess para n√£o bloquear o event loop.
        """
        if not hasattr(self, '_audio_buffer') or len(self._audio_buffer) == 0:
            return
        
        import tempfile
        from pathlib import Path
        
        try:
            # Salvar PCM raw em arquivo tempor√°rio
            fd, pcm_path = tempfile.mkstemp(suffix=".raw")
            with os.fdopen(fd, "wb") as f:
                f.write(self._audio_buffer)
            
            # Converter PCM 24kHz para WAV 16kHz (FreeSWITCH padr√£o)
            wav_path = pcm_path.replace(".raw", ".wav")
            
            # ffmpeg: PCM 24kHz mono -> WAV 16kHz mono
            # IMPORTANTE: Usar asyncio subprocess para n√£o bloquear
            process = await asyncio.create_subprocess_exec(
                "ffmpeg", "-y",
                "-f", "s16le",           # Input: PCM 16-bit signed
                "-ar", "24000",          # Input: 24kHz (OpenAI output)
                "-ac", "1",              # Input: mono
                "-i", pcm_path,
                "-ar", "16000",          # Output: 16kHz (FreeSWITCH)
                "-ac", "1",              # Output: mono
                wav_path,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.PIPE,
            )
            
            try:
                _, stderr = await asyncio.wait_for(process.communicate(), timeout=5.0)
            except asyncio.TimeoutError:
                process.kill()
                logger.warning("ffmpeg timeout, killing process")
                self._audio_buffer = bytearray()
                return
            
            if process.returncode == 0 and Path(wav_path).exists():
                # Tocar via uuid_broadcast no B-leg (humano)
                # Sem argumento = canal atual (B-leg), 'aleg' seria o cliente (errado)
                await self.esl.execute_api(
                    f"uuid_broadcast {self.b_leg_uuid} {wav_path} both"
                )
                logger.debug(f"Played {len(self._audio_buffer)} bytes to B-leg via TTS fallback")
            else:
                error_msg = stderr.decode()[:200] if stderr else "unknown"
                logger.warning(f"ffmpeg conversion failed: {error_msg}")
            
            # Limpar buffer
            self._audio_buffer = bytearray()
            
            # Cleanup temp files
            Path(pcm_path).unlink(missing_ok=True)
            # N√£o deletar wav_path imediatamente, FreeSWITCH precisa acessar
            
        except Exception as e:
            logger.error(f"Error flushing audio buffer: {e}")
            self._audio_buffer = bytearray()
    
    async def _cleanup(self) -> None:
        """Limpa recursos."""
        if self._ws:
            try:
                await self._ws.close()
            except Exception:
                pass
            self._ws = None
        
        if self._fs_sender_task:
            self._fs_sender_task.cancel()
            self._fs_sender_task = None
        
        if self._fs_ws:
            try:
                await self._fs_ws.close()
            except Exception:
                pass
            self._fs_ws = None
        
        if self._audio_ws_server:
            self._audio_ws_server.close()
            try:
                await self._audio_ws_server.wait_closed()
            except Exception:
                pass
            self._audio_ws_server = None
        
        # Opcional: parar stream no B-leg (pode derrubar canal em alguns ambientes)
        if os.getenv("REALTIME_BLEG_STREAM_STOP", "false").lower() in ("1", "true", "yes"):
            try:
                await self.esl.execute_api(f"uuid_audio_stream {self.b_leg_uuid} stop")
            except Exception:
                pass
        
        logger.debug("Realtime announcement session cleaned up")

    async def _handle_fs_ws(self, websocket: ServerConnection) -> None:
        """Recebe √°udio do FreeSWITCH (B-leg) e envia ao OpenAI."""
        if self._fs_ws:
            await websocket.close(1008, "Already connected")
            return
        
        self._fs_ws = websocket
        self._fs_connected.set()
        self._fs_rawaudio_sent = False
        self._fs_sender_task = asyncio.create_task(self._fs_sender_loop())
        
        try:
            async for message in websocket:
                if isinstance(message, bytes):
                    await self._handle_fs_audio(message)
                elif isinstance(message, str):
                    try:
                        data = json.loads(message)
                        if data.get("type") == "metadata":
                            logger.debug("B-leg metadata received", extra={"b_leg_uuid": self.b_leg_uuid})
                    except Exception:
                        pass
        except Exception as e:
            logger.info(f"B-leg ws closed: {e}")
        finally:
            if self._fs_sender_task:
                self._fs_sender_task.cancel()
                self._fs_sender_task = None

    async def _handle_fs_audio(self, audio_bytes: bytes) -> None:
        """Resample 16kHz -> 24kHz e envia ao OpenAI."""
        if not audio_bytes or not self._ws:
            return
        try:
            audio_24k = self._resampler_in.process(audio_bytes)
        except Exception:
            audio_24k = audio_bytes
        
        payload = {
            "type": "input_audio_buffer.append",
            "audio": base64.b64encode(audio_24k).decode("utf-8"),
        }
        try:
            await self._ws.send(json.dumps(payload))
        except Exception:
            pass

    async def _enqueue_audio_to_freeswitch(self, audio_bytes: bytes) -> None:
        """Enfileira √°udio do OpenAI para o FreeSWITCH (24kHz -> 16kHz)."""
        if not audio_bytes:
            return
        try:
            audio_16k = self._resampler_out.process(audio_bytes)
        except Exception:
            audio_16k = audio_bytes
        
        audio_16k = self._fs_audio_buffer.add(audio_16k)
        if not audio_16k:
            return
        
        # Quebrar em chunks de 20ms (640 bytes @16kHz PCM16)
        chunk_size = 640
        for i in range(0, len(audio_16k), chunk_size):
            chunk = audio_16k[i:i + chunk_size]
            try:
                await self._fs_audio_queue.put(chunk)
            except Exception:
                break

    async def _fs_sender_loop(self) -> None:
        """Envia √°udio para o FreeSWITCH via rawAudio + bin√°rio."""
        if not self._fs_ws:
            return
        
        try:
            if not self._fs_rawaudio_sent:
                header = {"type": "rawAudio", "data": {"sampleRate": 16000}}
                await self._fs_ws.send(json.dumps(header))
                self._fs_rawaudio_sent = True
            
            while self._running and self._fs_ws:
                chunk = await self._fs_audio_queue.get()
                await self._fs_ws.send(chunk)
        except Exception:
            pass
